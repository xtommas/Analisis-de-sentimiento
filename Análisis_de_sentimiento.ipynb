{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Análisis_de_sentimiento.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N32YUrO4OlnD"
      },
      "source": [
        "# **Análisis de sentimiento sobre un corpus de tweets en español**\n",
        "\n",
        "Para realizar este trabajo se utilizaron datasets provistos por la Sociedad Española del Procesado del Lenguaje Natural (SEPLN) para el Taller de Análisis de Sentimiento en español (TASS) en distintos años. Los archivos están en formato XML y poseen diversos campos, en este caso solo utilizaremos 'content' (el texto del tweet) y 'polarity' (la polaridad del tweet). Estas polaridades pueden ser:\n",
        "\n",
        "- P: positiva.\n",
        "- N: negativa.\n",
        "- NEU: neutral.\n",
        "- NONE: ninguna.\n",
        "\n",
        "### **Parsing de los archivos**\n",
        "\n",
        "Para poder usar los archivos, se realiza un parsing de los .xml a .csv para poder crear el dataframe con pandas. Esto se hace para los archivos correspondientes a los tweets de cada país. En caso de ejecutar el Google Colab, este paso no es necesario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7h1GAQQ1W7Fr",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('max_colwidth',1000)\n",
        "\n",
        "#Parseo de los tweets en el archivo TASS2019_country_UY_train.xml\n",
        "\n",
        "try:\n",
        "    general_tweets_corpus_train_UY = pd.read_csv('TASS\\\\general-tweets-train-tagged-UY.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\TASS2019_country_UY_train.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    general_tweets_corpus_train_UY = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content', 'polarity'], [tweet.content.text, tweet.sentiment.polarity.value.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        general_tweets_corpus_train_UY = general_tweets_corpus_train_UY.append(row_s)\n",
        "    general_tweets_corpus_train_UY.to_csv('TASS\\\\general-tweets-train-tagged-UY.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V-yOfVCFW7Fv",
        "colab": {}
      },
      "source": [
        "#Parseo de los tweets en el archivo TASS2019_country_UY_dev.xml\n",
        "\n",
        "try:\n",
        "    dev_tweets_corpus_train_UY = pd.read_csv('TASS\\\\dev-tweets-train-tagged-UY.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\TASS2019_country_UY_dev.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    dev_tweets_corpus_train_UY = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content', 'polarity'], [tweet.content.text, tweet.sentiment.polarity.value.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        dev_tweets_corpus_train_UY = dev_tweets_corpus_train_UY.append(row_s)\n",
        "    dev_tweets_corpus_train_UY.to_csv('TASS\\\\dev-tweets-train-tagged-UY.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LlnvvGROW7Fy",
        "colab": {}
      },
      "source": [
        "#Parseo de los tweets en el archivo TASS2019_country_CR_train.xml\n",
        "\n",
        "try:\n",
        "    general_tweets_corpus_train_CR = pd.read_csv('TASS\\\\general-tweets-train-tagged-CR.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\TASS2019_country_CR_train.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    general_tweets_corpus_train_CR = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content', 'polarity'], [tweet.content.text, tweet.sentiment.polarity.value.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        general_tweets_corpus_train_CR = general_tweets_corpus_train_CR.append(row_s)\n",
        "    general_tweets_corpus_train_CR.to_csv('TASS\\\\general-tweets-train-tagged-CR.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ElAWH5ODW7F1",
        "colab": {}
      },
      "source": [
        "#Parseo de los tweets en el archivo TASS2019_country_CR_dev.xml\n",
        "\n",
        "try:\n",
        "    dev_tweets_corpus_train_CR = pd.read_csv('TASS\\\\dev-tweets-train-tagged-CR.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\TASS2019_country_CR_dev.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    dev_tweets_corpus_train_CR = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content', 'polarity'], [tweet.content.text, tweet.sentiment.polarity.value.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        dev_tweets_corpus_train_CR = dev_tweets_corpus_train_CR.append(row_s)\n",
        "    dev_tweets_corpus_train_CR.to_csv('TASS\\\\dev-tweets-train-tagged-CR.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B8nMkAB2W7F5",
        "colab": {}
      },
      "source": [
        "#Parseo de los tweets en el archivo TASS2019_country_ES_train.xml\n",
        "\n",
        "try:\n",
        "    general_tweets_corpus_train_ES = pd.read_csv('TASS\\\\general-tweets-train-tagged-ES.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\TASS2019_country_ES_train.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    general_tweets_corpus_train_ES = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content', 'polarity'], [tweet.content.text, tweet.sentiment.polarity.value.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        general_tweets_corpus_train_ES = general_tweets_corpus_train_ES.append(row_s)\n",
        "    general_tweets_corpus_train_ES.to_csv('TASS\\\\general-tweets-train-tagged-ES.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PVkj7urjW7F9",
        "colab": {}
      },
      "source": [
        "#Parseo de los tweets en el archivo TASS2019_country_ES_dev.xml\n",
        "\n",
        "try:\n",
        "    dev_tweets_corpus_train_ES = pd.read_csv('TASS\\\\dev-tweets-train-tagged-ES.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\TASS2019_country_ES_dev.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    dev_tweets_corpus_train_ES = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content', 'polarity'], [tweet.content.text, tweet.sentiment.polarity.value.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        dev_tweets_corpus_train_ES = dev_tweets_corpus_train_ES.append(row_s)\n",
        "    dev_tweets_corpus_train_ES.to_csv('TASS\\\\dev-tweets-train-tagged-ES.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mMewaZwaW7GA",
        "colab": {}
      },
      "source": [
        "#Parseo de los tweets en el archivo TASS2019_country_MX_train.xml\n",
        "\n",
        "try:\n",
        "    general_tweets_corpus_train_MX = pd.read_csv('TASS\\\\general-tweets-train-tagged-MX.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\TASS2019_country_MX_train.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    general_tweets_corpus_train_MX = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content', 'polarity'], [tweet.content.text, tweet.sentiment.polarity.value.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        general_tweets_corpus_train_MX = general_tweets_corpus_train_MX.append(row_s)\n",
        "    general_tweets_corpus_train_MX.to_csv('TASS\\\\general-tweets-train-tagged-MX.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GtqS4L11W7GE",
        "colab": {}
      },
      "source": [
        "#Parseo de los tweets en el archivo TASS2019_country_MX_dev.xml\n",
        "\n",
        "try:\n",
        "    dev_tweets_corpus_train_MX = pd.read_csv('TASS\\\\dev-tweets-train-tagged-MX.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\TASS2019_country_MX_dev.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    dev_tweets_corpus_train_MX = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content', 'polarity'], [tweet.content.text, tweet.sentiment.polarity.value.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        dev_tweets_corpus_train_MX = dev_tweets_corpus_train_MX.append(row_s)\n",
        "    dev_tweets_corpus_train_MX.to_csv('TASS\\\\dev-tweets-train-tagged-MX.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xE2obT3yW7GH",
        "colab": {}
      },
      "source": [
        "#Parseo de los tweets en el archivo TASS2019_country_PE_train.xml\n",
        "\n",
        "try:\n",
        "    general_tweets_corpus_train_PE = pd.read_csv('TASS\\\\general-tweets-train-tagged-PE.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\TASS2019_country_PE_train.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    general_tweets_corpus_train_PE = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content', 'polarity'], [tweet.content.text, tweet.sentiment.polarity.value.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        general_tweets_corpus_train_PE = general_tweets_corpus_train_PE.append(row_s)\n",
        "    general_tweets_corpus_train_PE.to_csv('TASS\\\\general-tweets-train-tagged-PE.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KN5m8YwPW7GO",
        "colab": {}
      },
      "source": [
        "#Parseo de los tweets en el archivo TASS2019_country_PE_dev.xml\n",
        "\n",
        "try:\n",
        "    dev_tweets_corpus_train_PE = pd.read_csv('TASS\\\\dev-tweets-train-tagged-PE.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\TASS2019_country_PE_dev.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    dev_tweets_corpus_train_PE = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content', 'polarity'], [tweet.content.text, tweet.sentiment.polarity.value.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        dev_tweets_corpus_train_PE = dev_tweets_corpus_train_PE.append(row_s)\n",
        "    dev_tweets_corpus_train_PE.to_csv('TASS\\\\dev-tweets-train-tagged-PE.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3f7XH8agzMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Parseo de los tweets en el archivo intertass-CR-development-tagged.xml\n",
        "\n",
        "try:\n",
        "    intertass_dev_CR = pd.read_csv('TASS\\\\intertass-dev-CR.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\intertass-CR-development-tagged.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    intertass_dev_CR = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content', 'polarity'], [tweet.content.text, tweet.sentiment.polarity.value.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        intertass_dev_CR = intertass_dev_CR.append(row_s)\n",
        "    intertass_dev_CR.to_csv('TASS\\\\intertass-dev-CR.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVhKT0-vgzMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Parseo de los tweets en el archivo intertass-ES-development-tagged.xml\n",
        "\n",
        "try:\n",
        "    intertass_dev_ES = pd.read_csv('TASS\\\\intertass-dev-ES.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\intertass-ES-development-tagged.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    intertass_dev_ES = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content', 'polarity'], [tweet.content.text, tweet.sentiment.polarity.value.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        intertass_dev_ES = intertass_dev_ES.append(row_s)\n",
        "    intertass_dev_ES.to_csv('TASS\\\\intertass-dev-ES.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nND3M7nygzMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Parseo de los tweets en el archivo intertass-PE-development-tagged.xml\n",
        "\n",
        "try:\n",
        "    intertass_dev_PE = pd.read_csv('TASS\\\\intertass-dev-PE.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\intertass-PE-development-tagged.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    intertass_dev_PE = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content', 'polarity'], [tweet.content.text, tweet.sentiment.polarity.value.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        intertass_dev_PE = intertass_dev_PE.append(row_s)\n",
        "    intertass_dev_PE.to_csv('TASS\\\\intertass-dev-PE.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1t3143pgzMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Parseo de los tweets en el archivo intertass-CR-train-tagged.xml\n",
        "\n",
        "try:\n",
        "    intertass_train_CR = pd.read_csv('TASS\\\\intertass-train-CR.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\intertass-CR-train-tagged.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    intertass_train_CR = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content', 'polarity'], [tweet.content.text, tweet.sentiment.polarity.value.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        intertass_train_CR = intertass_train_CR.append(row_s)\n",
        "    intertass_train_CR.to_csv('TASS\\\\intertass-train-CR.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjgCM6iwgzMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Parseo de los tweets en el archivo intertass-ES-train-tagged.xml\n",
        "\n",
        "try:\n",
        "    intertass_train_ES = pd.read_csv('TASS\\\\intertass-train-ES.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\intertass-ES-train-tagged.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    intertass_train_ES = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content', 'polarity'], [tweet.content.text, tweet.sentiment.polarity.value.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        intertass_train_ES = intertass_train_ES.append(row_s)\n",
        "    intertass_train_ES.to_csv('TASS\\\\intertass-train-ES.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMdC_1GlgzMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Parseo de los tweets en el archivo intertass-PE-train-tagged.xml\n",
        "\n",
        "try:\n",
        "    intertass_train_PE = pd.read_csv('TASS\\\\intertass-train-PE.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\intertass-PE-train-tagged.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    intertass_train_PE = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content', 'polarity'], [tweet.content.text, tweet.sentiment.polarity.value.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        intertass_train_PE = intertass_train_PE.append(row_s)\n",
        "    intertass_train_PE.to_csv('TASS\\\\intertass-train-PE.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xiv7GJeegzM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Parseo de los tweets en el archivo politics-test-tagged.xml\n",
        "\n",
        "try:\n",
        "    politics_test = pd.read_csv('TASS\\\\politics-test.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\politics-test-tagged.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    politics_test = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content', 'polarity'], [tweet.content.text, tweet.sentiments.polarity.value.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        politics_test = politics_test.append(row_s)\n",
        "    politics_test.to_csv('TASS\\\\politics-test.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x0Y5jNIfOlnx",
        "colab": {}
      },
      "source": [
        "#Parseo de los tweets en el archivo TASS2019_country_UY_test.xml, para testear\n",
        "\n",
        "try:\n",
        "    tweets_test_UY = pd.read_csv('TASS\\\\tweets-test-tagged-UY.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\TASS2019_country_UY_test.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    tweets_test_UY = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content'], [tweet.content.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        tweets_test_UY = tweets_test_UY.append(row_s)\n",
        "    tweets_test_UY.to_csv('TASS\\\\tweets-test-tagged-UY.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MKpmqQ_cOlnz",
        "colab": {}
      },
      "source": [
        "#Parseo de los tweets en el archivo TASS2019_country_CR_test.xml, para testear\n",
        "\n",
        "try:\n",
        "    tweets_test_CR = pd.read_csv('TASS\\\\tweets-test-tagged-CR.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\TASS2019_country_CR_test.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    tweets_test_CR = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content'], [tweet.content.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        tweets_test_CR = tweets_test_CR.append(row_s)\n",
        "    tweets_test_CR.to_csv('TASS\\\\tweets-test-tagged-CR.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RSKpRxovOln2",
        "colab": {}
      },
      "source": [
        "#Parseo de los tweets en el archivo TASS2019_country_ES_test.xml, para testear\n",
        "\n",
        "try:\n",
        "    tweets_test_ES = pd.read_csv('TASS\\\\tweets-test-tagged-ES.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\TASS2019_country_ES_test.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    tweets_test_ES = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content'], [tweet.content.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        tweets_test_ES = tweets_test_ES.append(row_s)\n",
        "    tweets_test_ES.to_csv('TASS\\\\tweets-test-tagged-ES.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cmb4ovo0Oln4",
        "colab": {}
      },
      "source": [
        "#Parseo de los tweets en el archivo TASS2019_country_MX_test.xml, para testear\n",
        "\n",
        "try:\n",
        "    tweets_test_MX = pd.read_csv('TASS\\\\tweets-test-tagged-MX.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\TASS2019_country_MX_test.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    tweets_test_MX = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content'], [tweet.content.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        tweets_test_MX = tweets_test_MX.append(row_s)\n",
        "    tweets_test_MX.to_csv('TASS\\\\tweets-test-tagged-MX.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VyJVzdWAOln7",
        "colab": {}
      },
      "source": [
        "#Parseo de los tweets en el archivo TASS2019_country_PE_test.xml, para testear\n",
        "\n",
        "try:\n",
        "    tweets_test_PE = pd.read_csv('TASS\\\\tweets-test-tagged-PE.csv', encoding='utf-8')\n",
        "except:\n",
        "    from lxml import objectify\n",
        "    xml = objectify.parse(open('TASS\\\\TASS2019_country_PE_test.xml', encoding='utf-8'))\n",
        "    #sample tweet object\n",
        "    root = xml.getroot()\n",
        "    tweets_test_PE = pd.DataFrame(columns=('content', 'polarity'))\n",
        "    tweets = root.getchildren()\n",
        "    for i in range(0,len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        row = dict(zip(['content'], [tweet.content.text]))\n",
        "        row_s = pd.Series(row)\n",
        "        row_s.name = i\n",
        "        tweets_test_PE = tweets_test_PE.append(row_s)\n",
        "    tweets_test_PE.to_csv('TASS\\\\tweets-test-tagged-PE.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ynavk42m36g"
      },
      "source": [
        "En caso de ejecutar desde Google Colab, ejecutar la siguiente celda, hacer click en 'choose files' y seleccionar los 22 archivos .csv provistos. En otro caso, no es necesario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4dmuxrQXX07U",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "outputId": "20adf815-e9d3-453c-fcb3-08fc6608c3f7"
      },
      "source": [
        "\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "general_tweets_corpus_train_UY = pd.read_csv(io.BytesIO(uploaded['general-tweets-train-tagged-UY.csv']))\n",
        "general_tweets_corpus_train_CR = pd.read_csv(io.BytesIO(uploaded['general-tweets-train-tagged-CR.csv']))\n",
        "general_tweets_corpus_train_ES = pd.read_csv(io.BytesIO(uploaded['general-tweets-train-tagged-ES.csv']))\n",
        "general_tweets_corpus_train_MX = pd.read_csv(io.BytesIO(uploaded['general-tweets-train-tagged-MX.csv']))\n",
        "general_tweets_corpus_train_PE = pd.read_csv(io.BytesIO(uploaded['general-tweets-train-tagged-PE.csv']))\n",
        "\n",
        "dev_tweets_corpus_train_UY = pd.read_csv(io.BytesIO(uploaded['dev-tweets-train-tagged-UY.csv']))\n",
        "dev_tweets_corpus_train_CR = pd.read_csv(io.BytesIO(uploaded['dev-tweets-train-tagged-CR.csv']))\n",
        "dev_tweets_corpus_train_ES = pd.read_csv(io.BytesIO(uploaded['dev-tweets-train-tagged-ES.csv']))\n",
        "dev_tweets_corpus_train_MX = pd.read_csv(io.BytesIO(uploaded['dev-tweets-train-tagged-MX.csv']))\n",
        "dev_tweets_corpus_train_PE = pd.read_csv(io.BytesIO(uploaded['dev-tweets-train-tagged-PE.csv']))\n",
        "\n",
        "tweets_test_UY = pd.read_csv(io.BytesIO(uploaded['tweets-test-tagged-UY.csv']))\n",
        "tweets_test_CR = pd.read_csv(io.BytesIO(uploaded['tweets-test-tagged-CR.csv']))\n",
        "tweets_test_ES = pd.read_csv(io.BytesIO(uploaded['tweets-test-tagged-ES.csv']))\n",
        "tweets_test_MX = pd.read_csv(io.BytesIO(uploaded['tweets-test-tagged-MX.csv']))\n",
        "tweets_test_PE = pd.read_csv(io.BytesIO(uploaded['tweets-test-tagged-PE.csv']))\n",
        "\n",
        "intertass_dev_CR = pd.read_csv(io.BytesIO(uploaded['intertass-dev-CR.csv']))\n",
        "intertass_dev_ES = pd.read_csv(io.BytesIO(uploaded['intertass-dev-ES.csv']))\n",
        "intertass_dev_PE = pd.read_csv(io.BytesIO(uploaded['intertass-dev-PE.csv']))\n",
        "\n",
        "intertass_train_CR = pd.read_csv(io.BytesIO(uploaded['intertass-train-CR.csv']))\n",
        "intertass_train_ES = pd.read_csv(io.BytesIO(uploaded['intertass-train-ES.csv']))\n",
        "intertass_train_PE = pd.read_csv(io.BytesIO(uploaded['intertass-train-PE.csv']))\n",
        "\n",
        "politics_test = pd.read_csv(io.BytesIO(uploaded['politics-test.csv']))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5161f49f-01ab-482e-b53b-4502ed807c54\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5161f49f-01ab-482e-b53b-4502ed807c54\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving dev-tweets-train-tagged-CR.csv to dev-tweets-train-tagged-CR (2).csv\n",
            "Saving dev-tweets-train-tagged-ES.csv to dev-tweets-train-tagged-ES (2).csv\n",
            "Saving dev-tweets-train-tagged-MX.csv to dev-tweets-train-tagged-MX (2).csv\n",
            "Saving dev-tweets-train-tagged-PE.csv to dev-tweets-train-tagged-PE (2).csv\n",
            "Saving dev-tweets-train-tagged-UY.csv to dev-tweets-train-tagged-UY (2).csv\n",
            "Saving general-tweets-train-tagged-CR.csv to general-tweets-train-tagged-CR (2).csv\n",
            "Saving general-tweets-train-tagged-ES.csv to general-tweets-train-tagged-ES (2).csv\n",
            "Saving general-tweets-train-tagged-MX.csv to general-tweets-train-tagged-MX (2).csv\n",
            "Saving general-tweets-train-tagged-PE.csv to general-tweets-train-tagged-PE (2).csv\n",
            "Saving general-tweets-train-tagged-UY.csv to general-tweets-train-tagged-UY (2).csv\n",
            "Saving intertass-dev-CR.csv to intertass-dev-CR (2).csv\n",
            "Saving intertass-dev-ES.csv to intertass-dev-ES (2).csv\n",
            "Saving intertass-dev-PE.csv to intertass-dev-PE (2).csv\n",
            "Saving intertass-train-CR.csv to intertass-train-CR (2).csv\n",
            "Saving intertass-train-ES.csv to intertass-train-ES (2).csv\n",
            "Saving intertass-train-PE.csv to intertass-train-PE (2).csv\n",
            "Saving politics-test.csv to politics-test (2).csv\n",
            "Saving tweets-test-tagged-CR.csv to tweets-test-tagged-CR (2).csv\n",
            "Saving tweets-test-tagged-ES.csv to tweets-test-tagged-ES (2).csv\n",
            "Saving tweets-test-tagged-MX.csv to tweets-test-tagged-MX (2).csv\n",
            "Saving tweets-test-tagged-PE.csv to tweets-test-tagged-PE (2).csv\n",
            "Saving tweets-test-tagged-UY.csv to tweets-test-tagged-UY (2).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iDn0hUn0nM6m"
      },
      "source": [
        "Una vez realizado el parsing de todos los archivos de los datasets, formamos un solo gran corpus concatenando todos ellos. Se realiza una división en dos grupos:\n",
        "\n",
        "- **tweets_corpus**: que corresponde al conjunto de entrenamiento, por lo tanto posee el contenido de los tweets y su polaridad.\n",
        "- **tweets_test**: que corresponde al conjunto de testing y solo posee el  contenido de los tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5i-matBWW7GR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "b4134b46-1e88-4bd9-85de-4c155dafc029"
      },
      "source": [
        "tweets_corpus = pd.concat([\n",
        "    general_tweets_corpus_train_UY,\n",
        "    dev_tweets_corpus_train_UY,\n",
        "    general_tweets_corpus_train_CR,\n",
        "    dev_tweets_corpus_train_CR,\n",
        "    general_tweets_corpus_train_ES,\n",
        "    dev_tweets_corpus_train_ES,\n",
        "    general_tweets_corpus_train_MX,\n",
        "    dev_tweets_corpus_train_MX,\n",
        "    general_tweets_corpus_train_PE,\n",
        "    dev_tweets_corpus_train_PE,\n",
        "    intertass_dev_CR,\n",
        "    intertass_dev_ES,\n",
        "    intertass_dev_PE,\n",
        "    intertass_train_CR,\n",
        "    intertass_train_ES,\n",
        "    intertass_train_PE,\n",
        "    politics_test\n",
        "])\n",
        "\n",
        "tweets_test = pd.concat([\n",
        "    tweets_test_UY,\n",
        "    tweets_test_CR,\n",
        "    tweets_test_ES,\n",
        "    tweets_test_MX,\n",
        "    tweets_test_PE\n",
        "])\n",
        "\n",
        "tweets_corpus.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>@jose97angel ¿eliminada? Nooo, no la han elimi...</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>235</th>\n",
              "      <td>@anacubilla @ppopular lo primero q hay q hacer...</td>\n",
              "      <td>NEU</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>@mario_hart de parte de mi hermana ella dice q...</td>\n",
              "      <td>NEU</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>558</th>\n",
              "      <td>@YurikitoKSP que bien yuriko. Felicidades</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>@eunhyukeeprince no te preocupes ^^ como dije....</td>\n",
              "      <td>NEU</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2302</th>\n",
              "      <td>Independientes a favor de Pedro Costa Morata (...</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>829</th>\n",
              "      <td>Penoso lo de hoy, pensando en que hacer en el ...</td>\n",
              "      <td>NEU</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>@lirondos Ya lo tenía fichado y tengo ganas de...</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>Está navidad me la pasare durmiendo, para desp...</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>Sopa Negra, opción plato fuerte en la soda. Di...</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                content polarity\n",
              "195   @jose97angel ¿eliminada? Nooo, no la han elimi...        P\n",
              "235   @anacubilla @ppopular lo primero q hay q hacer...      NEU\n",
              "300   @mario_hart de parte de mi hermana ella dice q...      NEU\n",
              "558           @YurikitoKSP que bien yuriko. Felicidades        P\n",
              "292   @eunhyukeeprince no te preocupes ^^ como dije....      NEU\n",
              "2302  Independientes a favor de Pedro Costa Morata (...        P\n",
              "829   Penoso lo de hoy, pensando en que hacer en el ...      NEU\n",
              "81    @lirondos Ya lo tenía fichado y tengo ganas de...        P\n",
              "695   Está navidad me la pasare durmiendo, para desp...     NONE\n",
              "427   Sopa Negra, opción plato fuerte en la soda. Di...        P"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lwQcA9ysnuM5"
      },
      "source": [
        "Vemos que, entre todos los archivos, obtenemos 13879 tweets de entrenamiento y 7264 de testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hc2evqQ2W7GU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41169aaf-16cd-4f55-eb6a-d733044683a1"
      },
      "source": [
        "tweets_corpus.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13879, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yV65MnC6oAMS"
      },
      "source": [
        "### **Preprocesamiento del texto**\n",
        "\n",
        "El texto escrito en redes sociales tiende a ser informal, por lo que es susceptible a un uso más \"libre\" del lenguaje. Esto puede incluir: risas, palabras mal escritas, palabras sin tildes, etc. Se buscará obtener un vocabulario más consistente eliminando las tildes, las risas y contenido propio de la red social Twitter (como hashtags, links o nombres de usuario en menciones). Antes de realizar todo este preprocesamiento, nos quedamos solo con aquellos tweets que tengan polaridad positiva, negativa o neutral, es decir, eliminamos los tweets con polaridad NONE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kNlg5wxRW7GX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d02f0979-ca57-40c7-87e0-b4022dd1d241"
      },
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def elimina_tildes(s):\n",
        "    replacements = (\n",
        "        (\"á\", \"a\"),\n",
        "        (\"é\", \"e\"),\n",
        "        (\"í\", \"i\"),\n",
        "        (\"ó\", \"o\"),\n",
        "        (\"ú\", \"u\"),\n",
        "        (\"Á\", \"A\"),\n",
        "        (\"É\", \"E\"),\n",
        "        (\"Í\", \"I\"),\n",
        "        (\"Ó\", \"O\"),\n",
        "        (\"Ú\", \"U\"),\n",
        "        (\"à\", \"a\"),\n",
        "        (\"è\", \"e\"),\n",
        "        (\"ì\", \"i\"),\n",
        "        (\"ò\", \"o\"),\n",
        "        (\"ù\", \"u\"),\n",
        "    )\n",
        "    for a, b in replacements:\n",
        "        s = s.replace(a, b).replace(a.upper(), b.upper())\n",
        "    return s\n",
        "\n",
        "#Elimino los de porlaridad NONE\n",
        "tweets_corpus = tweets_corpus.query('polarity != \"NONE\"')\n",
        "\n",
        "#Elimino links\n",
        "tweets_corpus['content'] = tweets_corpus['content'].str.replace(r'^http.*$','')\n",
        "tweets_corpus['content'] = tweets_corpus['content'].str.replace(r'^https.*$','')\n",
        "\n",
        "#Elimino los nombres de ususario de las menciones(@usuario)\n",
        "tweets_corpus['content'] = tweets_corpus['content'].str.replace(r'(@[A-Za-z0-9_]+)','')\n",
        "\n",
        "#Elimino hashtags\n",
        "tweets_corpus['content'] = tweets_corpus['content'].str.replace(r'(#[A-Za-z0-9_]+)','')\n",
        "\n",
        "#Elimino tildes\n",
        "for index, row in tweets_corpus.iterrows():\n",
        "    row['content'] = elimina_tildes(row['content'])\n",
        "\n",
        "#Elimino risas\n",
        "tweets_corpus['content'] = tweets_corpus['content'].str.replace(r'ja(ja)+(j)*','')\n",
        "\n",
        "tweets_corpus['content'] = tweets_corpus['content'].str.replace(r'Ja(ja)+(j)*','') \n",
        "\n",
        "tweets_corpus['content'] = tweets_corpus['content'].str.replace(r'JA(JA)+(JA)*','')\n",
        "\n",
        "tweets_corpus['content'] = tweets_corpus['content'].str.replace(r'je(je)+(j)*','')\n",
        "\n",
        "tweets_corpus.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11318, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rW03lXEcpFb5"
      },
      "source": [
        "Vemos que, luego de este proceso, nos quedamos con 11318 tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GNb8wxGPW7Ga",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "a63cb72b-4695-435a-c04e-87f70f1fb652"
      },
      "source": [
        "tweets_corpus.sample(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Estoy en el \"Taco loco\"venden mas sopas,carnes...</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>576</th>\n",
              "      <td>que el domingo voy a votar a  para daros la p...</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>Que feo cuando se hacen cagones, pinches malcr...</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>Si tu dia esta amargo \"pues dale una movida\" a...</td>\n",
              "      <td>NEU</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>En mi cabeza, en mi cabeza vas dando vueltas! ...</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>352</th>\n",
              "      <td>por que siempre en las finales los hacen los ...</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>Buenos dias a todos!! Lamento tener que pospon...</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>Mi novia me compro un pajarito hermoso y como ...</td>\n",
              "      <td>NEU</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>738</th>\n",
              "      <td>Hoy no es un dia cualquiera, hoy cumplo 2 años...</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>760</th>\n",
              "      <td>, Os recomiendo leer este informe antes de ir ...</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>571</th>\n",
              "      <td>es genial estar incomunicada y desconectada de...</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>no los entiendo pero los extraño igual</td>\n",
              "      <td>NEU</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>Estoy escuchando el disco de  y me encanta Tal...</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>524</th>\n",
              "      <td>No es que ahora no sea feliz, pero antes lo er...</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2137</th>\n",
              "      <td>Esta tarde he ido al mitin de  ,es el unico pa...</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>Putamare!!.. El tio grito el gol en mi oido es...</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Genial ya estoy conectada</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>En pleno 2016 ya no deberia existir la siguien...</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>724</th>\n",
              "      <td>Otro para ti Javier, me ha hecho mucha ilusio...</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>372</th>\n",
              "      <td>Estoy en una montaña rusa emocional en la que ...</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                content polarity\n",
              "27    Estoy en el \"Taco loco\"venden mas sopas,carnes...        N\n",
              "576    que el domingo voy a votar a  para daros la p...        N\n",
              "395   Que feo cuando se hacen cagones, pinches malcr...        N\n",
              "280   Si tu dia esta amargo \"pues dale una movida\" a...      NEU\n",
              "499   En mi cabeza, en mi cabeza vas dando vueltas! ...        P\n",
              "352    por que siempre en las finales los hacen los ...        N\n",
              "112   Buenos dias a todos!! Lamento tener que pospon...        N\n",
              "426   Mi novia me compro un pajarito hermoso y como ...      NEU\n",
              "738   Hoy no es un dia cualquiera, hoy cumplo 2 años...        P\n",
              "760   , Os recomiendo leer este informe antes de ir ...        P\n",
              "571   es genial estar incomunicada y desconectada de...        P\n",
              "374              no los entiendo pero los extraño igual      NEU\n",
              "207   Estoy escuchando el disco de  y me encanta Tal...        P\n",
              "524   No es que ahora no sea feliz, pero antes lo er...        N\n",
              "2137  Esta tarde he ido al mitin de  ,es el unico pa...        P\n",
              "56    Putamare!!.. El tio grito el gol en mi oido es...        N\n",
              "10                           Genial ya estoy conectada         P\n",
              "27    En pleno 2016 ya no deberia existir la siguien...        N\n",
              "724    Otro para ti Javier, me ha hecho mucha ilusio...        P\n",
              "372   Estoy en una montaña rusa emocional en la que ...        N"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xP1xFVvXpZZW"
      },
      "source": [
        "### **Tokenización y Stemming**\n",
        "\n",
        "Utilizaremos las stopwords en español provistas por nltk, a las cuales también les quitaremos las tildes para que sean consistentes con el vocabulario de nuestro corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oipiMTOlW7Gd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "309c1945-101f-4f73-dcc5-657499b474ef"
      },
      "source": [
        "#descargar stopwords en español\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "spanish_stopwords = stopwords.words('spanish')\n",
        "\n",
        "espa_stopwords = []\n",
        "\n",
        "#elimino los tildes de las stopwords, para que coincidan con el dataset\n",
        "for word in spanish_stopwords:\n",
        "  espa_stopwords.append(elimina_tildes(word))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hpC9iNW5xg_y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f5660638-f0f1-46fc-cd5c-17f14c4f35dd"
      },
      "source": [
        "espa_stopwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['de',\n",
              " 'la',\n",
              " 'que',\n",
              " 'el',\n",
              " 'en',\n",
              " 'y',\n",
              " 'a',\n",
              " 'los',\n",
              " 'del',\n",
              " 'se',\n",
              " 'las',\n",
              " 'por',\n",
              " 'un',\n",
              " 'para',\n",
              " 'con',\n",
              " 'no',\n",
              " 'una',\n",
              " 'su',\n",
              " 'al',\n",
              " 'lo',\n",
              " 'como',\n",
              " 'mas',\n",
              " 'pero',\n",
              " 'sus',\n",
              " 'le',\n",
              " 'ya',\n",
              " 'o',\n",
              " 'este',\n",
              " 'si',\n",
              " 'porque',\n",
              " 'esta',\n",
              " 'entre',\n",
              " 'cuando',\n",
              " 'muy',\n",
              " 'sin',\n",
              " 'sobre',\n",
              " 'tambien',\n",
              " 'me',\n",
              " 'hasta',\n",
              " 'hay',\n",
              " 'donde',\n",
              " 'quien',\n",
              " 'desde',\n",
              " 'todo',\n",
              " 'nos',\n",
              " 'durante',\n",
              " 'todos',\n",
              " 'uno',\n",
              " 'les',\n",
              " 'ni',\n",
              " 'contra',\n",
              " 'otros',\n",
              " 'ese',\n",
              " 'eso',\n",
              " 'ante',\n",
              " 'ellos',\n",
              " 'e',\n",
              " 'esto',\n",
              " 'mi',\n",
              " 'antes',\n",
              " 'algunos',\n",
              " 'que',\n",
              " 'unos',\n",
              " 'yo',\n",
              " 'otro',\n",
              " 'otras',\n",
              " 'otra',\n",
              " 'el',\n",
              " 'tanto',\n",
              " 'esa',\n",
              " 'estos',\n",
              " 'mucho',\n",
              " 'quienes',\n",
              " 'nada',\n",
              " 'muchos',\n",
              " 'cual',\n",
              " 'poco',\n",
              " 'ella',\n",
              " 'estar',\n",
              " 'estas',\n",
              " 'algunas',\n",
              " 'algo',\n",
              " 'nosotros',\n",
              " 'mi',\n",
              " 'mis',\n",
              " 'tu',\n",
              " 'te',\n",
              " 'ti',\n",
              " 'tu',\n",
              " 'tus',\n",
              " 'ellas',\n",
              " 'nosotras',\n",
              " 'vosotros',\n",
              " 'vosotras',\n",
              " 'os',\n",
              " 'mio',\n",
              " 'mia',\n",
              " 'mios',\n",
              " 'mias',\n",
              " 'tuyo',\n",
              " 'tuya',\n",
              " 'tuyos',\n",
              " 'tuyas',\n",
              " 'suyo',\n",
              " 'suya',\n",
              " 'suyos',\n",
              " 'suyas',\n",
              " 'nuestro',\n",
              " 'nuestra',\n",
              " 'nuestros',\n",
              " 'nuestras',\n",
              " 'vuestro',\n",
              " 'vuestra',\n",
              " 'vuestros',\n",
              " 'vuestras',\n",
              " 'esos',\n",
              " 'esas',\n",
              " 'estoy',\n",
              " 'estas',\n",
              " 'esta',\n",
              " 'estamos',\n",
              " 'estais',\n",
              " 'estan',\n",
              " 'este',\n",
              " 'estes',\n",
              " 'estemos',\n",
              " 'esteis',\n",
              " 'esten',\n",
              " 'estare',\n",
              " 'estaras',\n",
              " 'estara',\n",
              " 'estaremos',\n",
              " 'estareis',\n",
              " 'estaran',\n",
              " 'estaria',\n",
              " 'estarias',\n",
              " 'estariamos',\n",
              " 'estariais',\n",
              " 'estarian',\n",
              " 'estaba',\n",
              " 'estabas',\n",
              " 'estabamos',\n",
              " 'estabais',\n",
              " 'estaban',\n",
              " 'estuve',\n",
              " 'estuviste',\n",
              " 'estuvo',\n",
              " 'estuvimos',\n",
              " 'estuvisteis',\n",
              " 'estuvieron',\n",
              " 'estuviera',\n",
              " 'estuvieras',\n",
              " 'estuvieramos',\n",
              " 'estuvierais',\n",
              " 'estuvieran',\n",
              " 'estuviese',\n",
              " 'estuvieses',\n",
              " 'estuviesemos',\n",
              " 'estuvieseis',\n",
              " 'estuviesen',\n",
              " 'estando',\n",
              " 'estado',\n",
              " 'estada',\n",
              " 'estados',\n",
              " 'estadas',\n",
              " 'estad',\n",
              " 'he',\n",
              " 'has',\n",
              " 'ha',\n",
              " 'hemos',\n",
              " 'habeis',\n",
              " 'han',\n",
              " 'haya',\n",
              " 'hayas',\n",
              " 'hayamos',\n",
              " 'hayais',\n",
              " 'hayan',\n",
              " 'habre',\n",
              " 'habras',\n",
              " 'habra',\n",
              " 'habremos',\n",
              " 'habreis',\n",
              " 'habran',\n",
              " 'habria',\n",
              " 'habrias',\n",
              " 'habriamos',\n",
              " 'habriais',\n",
              " 'habrian',\n",
              " 'habia',\n",
              " 'habias',\n",
              " 'habiamos',\n",
              " 'habiais',\n",
              " 'habian',\n",
              " 'hube',\n",
              " 'hubiste',\n",
              " 'hubo',\n",
              " 'hubimos',\n",
              " 'hubisteis',\n",
              " 'hubieron',\n",
              " 'hubiera',\n",
              " 'hubieras',\n",
              " 'hubieramos',\n",
              " 'hubierais',\n",
              " 'hubieran',\n",
              " 'hubiese',\n",
              " 'hubieses',\n",
              " 'hubiesemos',\n",
              " 'hubieseis',\n",
              " 'hubiesen',\n",
              " 'habiendo',\n",
              " 'habido',\n",
              " 'habida',\n",
              " 'habidos',\n",
              " 'habidas',\n",
              " 'soy',\n",
              " 'eres',\n",
              " 'es',\n",
              " 'somos',\n",
              " 'sois',\n",
              " 'son',\n",
              " 'sea',\n",
              " 'seas',\n",
              " 'seamos',\n",
              " 'seais',\n",
              " 'sean',\n",
              " 'sere',\n",
              " 'seras',\n",
              " 'sera',\n",
              " 'seremos',\n",
              " 'sereis',\n",
              " 'seran',\n",
              " 'seria',\n",
              " 'serias',\n",
              " 'seriamos',\n",
              " 'seriais',\n",
              " 'serian',\n",
              " 'era',\n",
              " 'eras',\n",
              " 'eramos',\n",
              " 'erais',\n",
              " 'eran',\n",
              " 'fui',\n",
              " 'fuiste',\n",
              " 'fue',\n",
              " 'fuimos',\n",
              " 'fuisteis',\n",
              " 'fueron',\n",
              " 'fuera',\n",
              " 'fueras',\n",
              " 'fueramos',\n",
              " 'fuerais',\n",
              " 'fueran',\n",
              " 'fuese',\n",
              " 'fueses',\n",
              " 'fuesemos',\n",
              " 'fueseis',\n",
              " 'fuesen',\n",
              " 'sintiendo',\n",
              " 'sentido',\n",
              " 'sentida',\n",
              " 'sentidos',\n",
              " 'sentidas',\n",
              " 'siente',\n",
              " 'sentid',\n",
              " 'tengo',\n",
              " 'tienes',\n",
              " 'tiene',\n",
              " 'tenemos',\n",
              " 'teneis',\n",
              " 'tienen',\n",
              " 'tenga',\n",
              " 'tengas',\n",
              " 'tengamos',\n",
              " 'tengais',\n",
              " 'tengan',\n",
              " 'tendre',\n",
              " 'tendras',\n",
              " 'tendra',\n",
              " 'tendremos',\n",
              " 'tendreis',\n",
              " 'tendran',\n",
              " 'tendria',\n",
              " 'tendrias',\n",
              " 'tendriamos',\n",
              " 'tendriais',\n",
              " 'tendrian',\n",
              " 'tenia',\n",
              " 'tenias',\n",
              " 'teniamos',\n",
              " 'teniais',\n",
              " 'tenian',\n",
              " 'tuve',\n",
              " 'tuviste',\n",
              " 'tuvo',\n",
              " 'tuvimos',\n",
              " 'tuvisteis',\n",
              " 'tuvieron',\n",
              " 'tuviera',\n",
              " 'tuvieras',\n",
              " 'tuvieramos',\n",
              " 'tuvierais',\n",
              " 'tuvieran',\n",
              " 'tuviese',\n",
              " 'tuvieses',\n",
              " 'tuviesemos',\n",
              " 'tuvieseis',\n",
              " 'tuviesen',\n",
              " 'teniendo',\n",
              " 'tenido',\n",
              " 'tenida',\n",
              " 'tenidos',\n",
              " 'tenidas',\n",
              " 'tened']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ld7N4QO0qRgW"
      },
      "source": [
        "Definimos la lista de símbolos a eliminar. Se añaden a ella los signos de apertura de pregunta y admiración, que se utilizan en el español."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2U0RP8_JW7Gh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "2cdf6632-3a09-4da3-c21c-448bc385c410"
      },
      "source": [
        "from string import punctuation\n",
        "non_words = list(punctuation)\n",
        "\n",
        "#añado los signos de pregunta y admiración usados en español\n",
        "non_words.extend(['¿', '¡'])\n",
        "non_words.extend(map(str,range(10)))\n",
        "non_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['!',\n",
              " '\"',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " '&',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " '+',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " ':',\n",
              " ';',\n",
              " '<',\n",
              " '=',\n",
              " '>',\n",
              " '?',\n",
              " '@',\n",
              " '[',\n",
              " '\\\\',\n",
              " ']',\n",
              " '^',\n",
              " '_',\n",
              " '`',\n",
              " '{',\n",
              " '|',\n",
              " '}',\n",
              " '~',\n",
              " '¿',\n",
              " '¡',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JCXdSQYuqmZG"
      },
      "source": [
        "Se definen las funciones de tokenización y stemming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vk0lOXkhW7Gk",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer     \n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# basado on http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "def stem_tokens(tokens, stemmer):\n",
        "    stemmed = []\n",
        "    for item in tokens:\n",
        "        stemmed.append(stemmer.stem(item))\n",
        "    return stemmed\n",
        "\n",
        "def tokenize(text):\n",
        "    # remover non words\n",
        "    text = ''.join([c for c in text if c not in non_words])\n",
        "    # tokenize\n",
        "    tokens =  word_tokenize(text)\n",
        "\n",
        "    # stem\n",
        "    try:\n",
        "        stems = stem_tokens(tokens, stemmer)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(text)\n",
        "        stems = ['']\n",
        "    return stems\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wSJocX8lqxdt"
      },
      "source": [
        "### **Modelo: entrenamiento**\n",
        "\n",
        "Tomaremos a este problema como un problema de clasificación de 3 clases (positivo, negativo, neutral). Por lo tanto, creamos una nueva columna 'polarity_num' donde realizamos la siguiente asignación numérica para cada polaridad:\n",
        "\n",
        "- 0: polaridad negativa.\n",
        "- 1: polaridad positiva.\n",
        "- 2: polaridad neutral."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Zs7SAgXW7Gm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "45f218fa-1125-414a-b62d-b1313637428a"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Tres clases\n",
        "# N = 0\n",
        "# P = 1\n",
        "# NEU = 2\n",
        "tweets_corpus['polarity_num'] = 0\n",
        "tweets_corpus.polarity_num[tweets_corpus.polarity.isin(['P'])] = 1\n",
        "tweets_corpus.polarity_num[tweets_corpus.polarity.isin(['NEU'])] = 2\n",
        "\n",
        "tweets_corpus.polarity_num.value_counts(normalize = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.437180\n",
              "1    0.341050\n",
              "2    0.221771\n",
              "Name: polarity_num, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xpTbZ-KyrqnO"
      },
      "source": [
        "A continuación se presenta el modelo utilizado: Linear Support Vector Classifier (LinearSVC) provisto por scikit learn y una representación de bolsa de palabras con un vectorizador CountVectorizer.\n",
        "\n",
        "Se utilizó Grid Search para la búsqueda de los hiperparámetros y accuracy como medida de score, debido a que se trata de un problema con múltiples clases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v8u8s6s0W7Gt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "8d888235-a033-4661-cb78-2035bd94c604"
      },
      "source": [
        "vectorizer = CountVectorizer(\n",
        "                analyzer = 'word',\n",
        "                tokenizer = tokenize,\n",
        "                lowercase = True,\n",
        "                stop_words = espa_stopwords)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vect', vectorizer),\n",
        "    ('cls', LinearSVC()),\n",
        "])\n",
        "\n",
        "\n",
        "parameters = {\n",
        "    'vect__max_df': (0.5, 1.0, 1.9),\n",
        "    'vect__min_df': (5, 10, 20),\n",
        "    'vect__max_features': (500, 1000),\n",
        "    'vect__ngram_range': ((1, 1), (1, 2), (1,4)), \n",
        "    'cls__C': (0.003, 0.02, 0.2),\n",
        "    'cls__loss': ('hinge', 'squared_hinge'),\n",
        "    'cls__max_iter': (500, 20000)\n",
        "}\n",
        "\n",
        "import sklearn\n",
        "\n",
        "nltk.download('punkt')\n",
        "  \n",
        "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1 , scoring='accuracy')\n",
        "grid_search.fit(tweets_corpus.content, tweets_corpus.polarity_num)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estam', 'estand', 'estari', 'estem', 'estuv', 'estuvier', 'estuvies', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habi', 'habr', 'habri', 'hast', 'hem', 'hub', 'hubier', 'hubies', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seam', 'sent', 'ser', 'seri', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambi', 'tant', 'ten', 'tendr', 'tendri', 'teng', 'teni', 'tien', 'tod', 'tuv', 'tuvier', 'tuvies', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=None, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('vect',\n",
              "                                        CountVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.int64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=1.0,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=1,\n",
              "                                                        ngram_range=(1, 1),\n",
              "                                                        preprocessor=None,\n",
              "                                                        stop_words=['de', 'la',\n",
              "                                                                    'que', 'el',\n",
              "                                                                    'en', 'y',\n",
              "                                                                    'a', 'los'...\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'cls__C': (0.003, 0.02, 0.2),\n",
              "                         'cls__loss': ('hinge', 'squared_hinge'),\n",
              "                         'cls__max_iter': (500, 20000),\n",
              "                         'vect__max_df': (0.5, 1.0, 1.9),\n",
              "                         'vect__max_features': (500, 1000),\n",
              "                         'vect__min_df': (5, 10, 20),\n",
              "                         'vect__ngram_range': ((1, 1), (1, 2), (1, 4))},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='accuracy', verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S4e3vr4ZW7Gv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "65b5e869-b053-443e-b635-c9516b69a31f"
      },
      "source": [
        "grid_search.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cls__C': 0.2,\n",
              " 'cls__loss': 'hinge',\n",
              " 'cls__max_iter': 20000,\n",
              " 'vect__max_df': 0.5,\n",
              " 'vect__max_features': 1000,\n",
              " 'vect__min_df': 10,\n",
              " 'vect__ngram_range': (1, 1)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wFwOV36Mxmeh"
      },
      "source": [
        "Se utiliza crossvalidation para ver la performance del modelo. A pesar de el uso de Grid Search, se obtuvieron mejores resultados ajustando los hiperparámetros de forma manual. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lBIptd7rW7Gy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "855db81d-3e10-47e6-8284-37b185c07a10"
      },
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "model = LinearSVC(C=23, loss='squared_hinge',max_iter=1000000,multi_class='ovr',\n",
        "              random_state=None,\n",
        "              penalty='l1',\n",
        "              tol=0.0001,\n",
        "              dual=False\n",
        ")\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    analyzer = 'word',\n",
        "    tokenizer = tokenize,\n",
        "    lowercase = True,\n",
        "    stop_words = espa_stopwords,\n",
        "    min_df = 5,\n",
        "    max_df = 1.0,\n",
        "    ngram_range=(1, 4),\n",
        "    max_features=10000\n",
        ")\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(\n",
        "            analyzer = 'word', # n-gramas de palabras\n",
        "            tokenizer = tokenize,\n",
        "            lowercase = True,\n",
        "            stop_words = espa_stopwords,\n",
        "            min_df = 5, # Añade al vocabulario palabras que aparezcan en al menos 5 tweets\n",
        "            max_df = 1.0, # Ignora palabras que aparezcan en todos los tweets\n",
        "            ngram_range=(1, 4), # Se usan desde unigramas a cuatrigramas\n",
        "            max_features=10000\n",
        "            )),\n",
        "    ('cls', LinearSVC(C=23, loss='squared_hinge',max_iter=100000,multi_class='ovr',\n",
        "             random_state=None,\n",
        "             penalty='l1',\n",
        "             tol=0.0001,\n",
        "             dual=False\n",
        "             )),\n",
        "])\n",
        "\n",
        "corpus_data_features = vectorizer.fit_transform(tweets_corpus.content)\n",
        "corpus_data_features_nd = corpus_data_features.toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estam', 'estand', 'estari', 'estem', 'estuv', 'estuvier', 'estuvies', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habi', 'habr', 'habri', 'hast', 'hem', 'hub', 'hubier', 'hubies', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seam', 'sent', 'ser', 'seri', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambi', 'tant', 'ten', 'tendr', 'tendri', 'teng', 'teni', 'tien', 'tod', 'tuv', 'tuvier', 'tuvies', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BGs9b8cPqT-D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b536384e-2b65-43eb-9392-820089af462c"
      },
      "source": [
        "scores = cross_val_score(\n",
        "    model,\n",
        "    corpus_data_features_nd[0:len(tweets_corpus)],\n",
        "    y=tweets_corpus.polarity_num,\n",
        "    scoring='accuracy',\n",
        "    cv=5\n",
        "    )\n",
        "\n",
        "scores.mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7104487773039635"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ivh4ZPteteXs"
      },
      "source": [
        "Vemos que la accuracy producida es de 71%, que no está mal, pero no es muy alta. Esto puede deberse a la mezcla de vocabulario de distintos países, el uso de ironía u otros modos de expresión donde el texto se debe interpretar más allá del significado real de las palabras, palabras mal escritas que producen inconsistencia en el vocabulario, la relativamente poca cantidad de tweets, la cantidad de temáticas distintas en el texto, entre otros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CErD8ciQ1p2H"
      },
      "source": [
        "### **Modelo: predicciones de polaridad**\n",
        "\n",
        "aplicamos el mismo preprocesamiento al conjunto de tweets de prueba y utilizamos el modelo para realizar preedicciones sobre la polaridad de los mismos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YPYhprAkOlol",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "440110ac-4a0a-42ca-910a-1f50ed2b5f72"
      },
      "source": [
        "#Elimino links\n",
        "tweets_test['content'] = tweets_test['content'].str.replace(r'^http.*$','')\n",
        "tweets_test['content'] = tweets_test['content'].str.replace(r'^https.*$','')\n",
        "\n",
        "#Elimino los nombres de ususario de las menciones(@usuario)\n",
        "tweets_test['content'] = tweets_test['content'].str.replace(r'(@[A-Za-z0-9_]+)','')\n",
        "\n",
        "#Elimino hashtags\n",
        "tweets_test['content'] = tweets_test['content'].str.replace(r'(#[A-Za-z0-9_]+)','')\n",
        "\n",
        "#Elimino tildes\n",
        "tweets_test['content'] = tweets_test['content'].apply(elimina_tildes)\n",
        "\n",
        "#Elimino risas\n",
        "tweets_test['content'] = tweets_test['content'].str.replace(r'ja(ja)+(j)*','')\n",
        "\n",
        "tweets_test['content'] = tweets_test['content'].str.replace(r'Ja(ja)+(j)*','') \n",
        "\n",
        "tweets_test['content'] = tweets_test['content'].str.replace(r'JA(JA)+(JA)*','')\n",
        "\n",
        "tweets_test['content'] = tweets_test['content'].str.replace(r'je(je)+(j)*','')\n",
        "\n",
        "tweets_test.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7264, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JHmEv1aeOlon",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c23afcdc-3936-41c5-9536-0406caa1637e"
      },
      "source": [
        "pipeline.fit(tweets_corpus.content, tweets_corpus.polarity_num)\n",
        "tweets_test['polarity'] = pipeline.predict(tweets_test.content)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estam', 'estand', 'estari', 'estem', 'estuv', 'estuvier', 'estuvies', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habi', 'habr', 'habri', 'hast', 'hem', 'hub', 'hubier', 'hubies', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seam', 'sent', 'ser', 'seri', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambi', 'tant', 'ten', 'tendr', 'tendri', 'teng', 'teni', 'tien', 'tod', 'tuv', 'tuvier', 'tuvies', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MOgqIk8zb9f1"
      },
      "source": [
        "Vemos los resultados, recordemos que las clases son:\n",
        "\n",
        "- 0: polaridad negativa.\n",
        "- 1: polaridad positiva.\n",
        "- 2: polaridad neutral."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AJAE6yiTOloq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "85d5aeaa-1506-4c2e-ad8e-3be311cb108a"
      },
      "source": [
        "tweets_test.sample(50)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e474f2b217d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tweets_test' is not defined"
          ]
        }
      ]
    }
  ]
}